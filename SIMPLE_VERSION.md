# Упрощенная архитектура ChatCopilot на базе vLLM

Этот документ описывает упрощенную, экономически эффективную и LLM-центричную архитектуру для ChatCopilot. Цель — отказаться от излишне сложной и дорогой микросервисной системы в пользу единого мощного компонента — языковой модели, доступной через vLLM.

## 1. Бизнес-описание

### Основная идея

Вместо того чтобы строить и поддерживать сложную экосистему из множества баз данных (Redis, Pinecone, Elasticsearch), микросервисов и узкоспециализированных ML-моделей, мы используем **единую мощную языковую модель (LLM) как центральный "мозг"** системы. Все интеллектуальные задачи — от ответов на вопросы до суммаризации и извлечения фактов — решаются с помощью грамотно составленных запросов (промптов) к этой модели.

### Ключевые преимущества для бизнеса

1.  **Радикальное снижение затрат**:
    *   **Инфраструктура**: Мы устраняем необходимость в дорогостоящих и сложных для поддержки сервисах, таких как Redis, Pinecone и Elasticsearch. Основные расходы — это сервер для vLLM и база данных Supabase.
    *   **Разработка и поддержка**: Стоимость разработки и поддержки сокращается в разы. Команде не нужно тратить время на администрирование 4-х разных баз данных, обучение кастомных ML-моделей и отладку сложного взаимодействия между микросервисами.

2.  **Ускорение разработки (Time-to-Market)**:
    *   Простая архитектура означает, что новые функции можно внедрять и тестировать гораздо быстрее. Фокус смещается с поддержки инфраструктуры на улучшение пользовательского опыта и логики бота.

3.  **Простота и надежность**:
    *   Управлять двумя основными компонентами (бот + vLLM) и единой базой данных (Supabase) значительно проще, чем целым "зоопарком" технологий. Это снижает количество точек отказа и упрощает диагностику проблем.

4.  **Гибкость и масштабируемость**:
    *   Заменить или обновить языковую модель на более мощную в будущем можно без перестройки всей архитектуры. Достаточно обновить модель на vLLM сервере.

## 2. Техническое описание

### Компоненты архитектуры

1.  **ChatCopilot Bot**: Основное приложение на `aiogram`, которое управляет логикой, состоянием и взаимодействием с пользователем.
2.  **vLLM Server**: Единый сервер, предоставляющий доступ к мощной языковой модели (например, `Qwen/Qwen3-4B`). Он становится единственным источником для всех интеллектуальных задач.
3.  **Supabase (PostgreSQL)**: Единое хранилище данных. Используется для хранения:
    *   Всех сообщений из чатов.
    *   Информации о командах и пользователях.
    *   Сгенерированных резюме и отчетов.
    *   Настроек (например, системных сообщений для команд).

### Сценарии работы

#### Сценарий 1: Ответ на вопрос пользователя

1.  **Запрос**: Пользователь задает вопрос в режиме чата.
2.  **Поиск контекста**: Бот выполняет запрос к **Supabase**, чтобы найти релевантную историю сообщений. Это может быть:
    *   Простой поиск последних N сообщений в чате команды.
    *   Полнотекстовый поиск по ключевым словам из вопроса пользователя.
3.  **Формирование промпта (Prompt Engineering)**: Бот создает подробный промпт для LLM.
    *   **Пример промпта:**
        ```
        Ты — ChatCopilot, ИИ-ассистент. Проанализируй приведенный ниже контекст из переписки команды и ответь на вопрос пользователя.

        [КОНТЕКСТ ИЗ SUPABASE]
        ...сообщения...

        [ВОПРОС ПОЛЬЗОВАТЕЛЯ]
        ...вопрос...

        Ответ должен быть четким, по существу и на русском языке.
        ```
4.  **Запрос к vLLM**: Бот отправляет этот промпт на `vLLM` сервер.
5.  **Генерация ответа**: LLM обрабатывает промпт и генерирует ответ.
6.  **Отправка пользователю**: Бот получает ответ и пересылает его пользователю.

#### Сценарий 2: Автоматическая суммаризация (фоновая задача)

1.  **Триггер**: По расписанию (например, раз в день) запускается фоновая задача.
2.  **Получение данных**: Задача запрашивает из **Supabase** все сообщения команды за последние 24 часа.
3.  **Формирование промпта для суммаризации**:
    *   **Пример промпта:**
        ```
        Ты — ИИ-аналитик. Сделай краткую сводку (резюме) по следующему диалогу. Выдели основные темы обсуждения, принятые решения и поставленные задачи.

        [ДИАЛОГ ЗА ДЕНЬ ИЗ SUPABASE]
        ...сообщения...

        Сводка:
        ```
4.  **Запрос к vLLM**: Промпт отправляется на `vLLM` сервер.
5.  **Сохранение результата**: LLM генерирует сводку, которая сохраняется в отдельную таблицу в **Supabase** для быстрого доступа в будущем.

### Схема архитектуры

```mermaid
graph TD
    subgraph "Пользователь"
        A[Клиент Telegram]
    end

    subgraph "Инфраструктура ChatCopilot"
        B(Бот на Aiogram)
        C(Сервер vLLM)
        D[База данных Supabase]
    end

    A -->|Сообщения, команды| B
    B -->|Ответы, интерфейс| A

    B <-->|1. Запрос контекста<br>3. Сохранение данных| D
    B <-->|2. Запрос к LLM (вопросы, суммаризация)| C
```

## 3. Сравнение качества: Сложная vs. Упрощенная архитектура

| Критерий | Сложная архитектура (микросервисы, multi-DB) | Упрощенная архитектура (LLM-centric) |
| :--- | :--- | :--- |
| **Сильные стороны** | **Высокая точность в узких задачах.** Специализированные модели (NER, классификаторы) могут давать >90% точности на своей конкретной задаче. Скорость ответа может быть выше за счет пред-индексации. | **Гибкость и универсальность.** LLM способна понимать нюансы, обрабатывать нестандартные запросы и делать логические выводы, которые не под силу узкоспециализированным моделям. Качество ответа более "человечное". |
| **Слабые стороны** | **"Хрупкость".** Качество всей системы зависит от каждого компонента. Ошибка в одном сервисе (например, неправильная классификация) ломает всю цепочку. Плохо справляется с вопросами, не укладывающимися в рамки заложенных категорий. | **Зависимость от мощности LLM.** Качество напрямую зависит от выбранной модели. Может быть чуть медленнее, так как обработка происходит в реальном времени, а не извлекается из кеша. |
| **Общая оценка качества** | **Высокая точность, низкая гибкость.** Система хорошо решает только те задачи, на которые была явно спроектирована и обучена. Может давать нерелевантные результаты на неожиданных запросах. | **Высокая релевантность, высокая гибкость.** Система хорошо справляется с широким спектром запросов. Качество ответа более стабильно и менее подвержено "хрупкости" архитектуры. |

### Итог

Упрощенная архитектура обеспечивает **сопоставимое или даже лучшее качество ответов на широкий круг вопросов** при многократном сокращении сложности и стоимости. Она делает систему более надежной, гибкой и готовой к будущим улучшениям в области языковых моделей. 