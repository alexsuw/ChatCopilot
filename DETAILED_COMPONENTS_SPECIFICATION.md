# –î–µ—Ç–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ 

> [!WARNING]
> **–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —É—Å—Ç–∞—Ä–µ–ª.**
> 
> –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –≥–∏–±—Ä–∏–¥–Ω–∞—è RAG-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±—ã–ª–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –∏–∑–±—ã—Ç–æ—á–Ω–æ —Å–ª–æ–∂–Ω–æ–π –∏ –¥–æ—Ä–æ–≥–æ–π –¥–ª—è —Ç–µ–∫—É—â–∏—Ö –∑–∞–¥–∞—á –ø—Ä–æ–µ–∫—Ç–∞.
>
> **–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –∏ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –æ–ø–∏—Å–∞–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ: [`SIMPLE_VERSION.md`](./SIMPLE_VERSION.md)**
>
> –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –∫–∞–∫ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ –≤–µ—Ä–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞.

---

### –ê—Ä—Ö–∏–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)

–ê—Ä—Ö–∏–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏, –Ω–æ **–Ω–µ –¥–æ–ª–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è** –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.

<details>
<summary>–ù–∞–∂–º–∏—Ç–µ, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à—É—é —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é</summary>

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 1: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

### üéØ **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**
–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –¥–æ—Å—Ç—É–ø–∞ –∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç–∏.

### üèóÔ∏è **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É—Ä–æ–≤–Ω–∏**

#### **–£—Ä–æ–≤–µ–Ω—å 1: –ñ–∏–≤–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (Live Context)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# Redis —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
{
    "live_context:team_123:2024-01-15:14": {
        "messages": [
            {
                "id": "msg_001",
                "content": "–û–±—Å—É–∂–¥–∞–µ–º –Ω–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö",
                "author": "alex",
                "timestamp": "2024-01-15T14:30:00Z",
                "importance": 0.85,
                "entities": ["–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"],
                "thread_id": "thread_001"
            }
        ],
        "summary": "–û–±—Å—É–∂–¥–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ë–î",
        "participants": ["alex", "maria", "ivan"],
        "active_topics": ["database", "architecture"]
    }
}
```

**Self-hosted –≤–∞—Ä–∏–∞–Ω—Ç:**
- **Redis Cluster**: 3 –Ω–æ–¥—ã –ø–æ 16GB RAM
- **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ TTL**: 3 —á–∞—Å–∞ –¥–ª—è live –¥–∞–Ω–Ω—ã—Ö
- **Persistence**: RDB snapshots –∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: ~$450/–º–µ—Å—è—Ü
- **–ü–ª—é—Å—ã**: –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å, –Ω–∏–∑–∫–∞—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
- **–ú–∏–Ω—É—Å—ã**: –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è

**API –≤–∞—Ä–∏–∞–Ω—Ç:**
- **Redis Cloud**: Managed service
- **–ê–≤—Ç–æ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ**: –¥–æ 100GB –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: ~$600/–º–µ—Å—è—Ü
- **–ü–ª—é—Å—ã**: –ù–µ—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è, SLA 99.9%
- **–ú–∏–Ω—É—Å—ã**: –í—ã—à–µ —Å—Ç–æ–∏–º–æ—Å—Ç—å, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

#### **–£—Ä–æ–≤–µ–Ω—å 2: –î–Ω–µ–≤–Ω—ã–µ —Å–≤–æ–¥–∫–∏ (Daily Summaries)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# PostgreSQL —Ç–∞–±–ª–∏—Ü–∞ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏
CREATE TABLE daily_summaries (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    team_id UUID NOT NULL,
    date DATE NOT NULL,
    summary TEXT NOT NULL,
    key_decisions TEXT[],
    active_participants TEXT[],
    main_topics TEXT[],
    sentiment_score FLOAT DEFAULT 0.0,
    activity_level INTEGER DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT unique_team_date UNIQUE(team_id, date)
);

-- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX idx_daily_summaries_team_date ON daily_summaries(team_id, date DESC);
CREATE INDEX idx_daily_summaries_topics ON daily_summaries USING GIN(main_topics);
```

**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–æ–∫:**
```python
class DailySummaryGenerator:
    def __init__(self, summarization_model):
        self.model = summarization_model
        
    async def generate_daily_summary(self, team_id: str, date: datetime.date):
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–Ω—è
        messages = await self.get_day_messages(team_id, date)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–º–∞–º
        topic_groups = self.group_by_topics(messages)
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—é–º–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
        topic_summaries = []
        for topic, msgs in topic_groups.items():
            summary = await self.model.summarize(msgs, max_length=200)
            topic_summaries.append({
                'topic': topic,
                'summary': summary,
                'message_count': len(msgs)
            })
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ –¥–Ω—è
        daily_summary = await self.create_daily_overview(topic_summaries)
        
        return daily_summary
```

**–ë–µ–∑ –æ–±—É—á–µ–Ω–∏—è (–≥–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏):**
- **T5-small**: –•–æ—Ä–æ—à–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- **mT5-base**: –ú–Ω–æ–≥–æ—è–∑—ã—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
- **BART**: –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 75-80% –±–µ–∑ fine-tuning

**–° –æ–±—É—á–µ–Ω–∏–µ–º (fine-tuned):**
- **–î–∞—Ç–∞—Å–µ—Ç**: 10K –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–≤–æ–¥–æ–∫ –∫–æ–º–∞–Ω–¥
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 2-3 –¥–Ω—è –Ω–∞ V100
- **–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è**: ~$500
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 85-90% –ø–æ—Å–ª–µ fine-tuning

#### **–£—Ä–æ–≤–µ–Ω—å 3: –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–µ–∫—Å—ã (Topic Indexes)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
class TopicDetector:
    def __init__(self):
        self.clustering_model = None
        self.topic_embeddings = {}
        
    def detect_topics(self, messages: List[str]) -> Dict[str, List[str]]:
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        embeddings = self.get_embeddings(messages)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ —Ç–µ–º–∞–º
        clusters = self.cluster_messages(embeddings)
        
        # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ç–µ–º—ã –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        topics = {}
        for cluster_id, message_ids in clusters.items():
            topic_name = self.generate_topic_name(message_ids)
            topics[topic_name] = message_ids
            
        return topics
```

**Self-hosted —Ä–µ—à–µ–Ω–∏–µ:**
- **Sentence-BERT**: –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- **HDBSCAN**: –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
- **KeyBERT**: –î–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: ~$200/–º–µ—Å—è—Ü (GPU —Å–µ—Ä–≤–µ—Ä)

**API —Ä–µ—à–µ–Ω–∏–µ:**
- **OpenAI Embeddings**: text-embedding-3-small
- **Pinecone**: –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: ~$300/–º–µ—Å—è—Ü
- **–ü–ª—é—Å—ã**: –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø—Ä–æ—Å—Ç–æ—Ç–∞
- **–ú–∏–Ω—É—Å—ã**: –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç API

#### **–£—Ä–æ–≤–µ–Ω—å 4: –ì–ª—É–±–æ–∫–∏–π –∞—Ä—Ö–∏–≤ (Deep Archive)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# –ê—Ä—Ö–∏–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –∫–æ–º–ø—Ä–µ—Å—Å–∏–µ–π
class DeepArchiveManager:
    def __init__(self):
        self.compression_enabled = True
        self.archive_threshold_days = 30
        
    async def archive_old_data(self, team_id: str):
        # –ù–∞—Ö–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ä—à–µ –ø–æ—Ä–æ–≥–∞
        old_data = await self.find_old_data(team_id)
        
        # –ö–æ–º–ø—Ä–µ—Å—Å–∏—è –∏ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ
        for data_batch in old_data:
            compressed = self.compress_data(data_batch)
            await self.store_in_archive(team_id, compressed)
            
    def compress_data(self, data: List[Dict]) -> bytes:
        # –£–¥–∞–ª—è–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        essential_data = self.extract_essential_info(data)
        
        # –°–∂–∏–º–∞–µ–º —Å –ø–æ–º–æ—â—å—é gzip
        return gzip.compress(json.dumps(essential_data).encode())
```

---

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 2: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

### üéØ **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ** 
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö.

### ü§ñ **ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã**

#### **2.1 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏–π**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class MessageImportanceClassifier:
    def __init__(self, model_path: str = None):
        if model_path:
            self.model = self.load_model(model_path)
        else:
            self.model = self.load_pretrained_model()
    
    def classify_importance(self, message: str) -> float:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = self.extract_features(message)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å (0.0 - 1.0)
        importance_score = self.model.predict(features)[0]
        
        return importance_score
    
    def extract_features(self, message: str) -> np.ndarray:
        return np.array([
            self.has_decision_keywords(message),
            self.has_question_marks(message),
            self.message_length(message),
            self.has_mentions(message),
            self.has_urls(message),
            self.sentiment_score(message),
            self.urgency_indicators(message)
        ])
```

**–ë–µ–∑ –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–∞–≤–∏–ª–∞ + –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏):**
```python
class RuleBasedImportanceClassifier:
    def __init__(self):
        self.decision_keywords = ['—Ä–µ—à–∏–ª–∏', '–ø—Ä–∏–Ω–∏–º–∞–µ–º', '—É—Ç–≤–µ—Ä–∂–¥–∞–µ–º', '–¥–µ–ª–∞–µ–º']
        self.question_keywords = ['–∫–∞–∫', '—á—Ç–æ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É']
        self.urgent_keywords = ['—Å—Ä–æ—á–Ω–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', 'asap', '–∫—Ä–∏—Ç–∏—á–Ω–æ']
    
    def classify_importance(self, message: str) -> float:
        score = 0.5  # –±–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if any(kw in message.lower() for kw in self.decision_keywords):
            score += 0.3
        
        if any(kw in message.lower() for kw in self.urgent_keywords):
            score += 0.2
            
        # –î–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
        if len(message) > 100:
            score += 0.1
            
        return min(score, 1.0)
```

**–° –æ–±—É—á–µ–Ω–∏–µ–º (fine-tuned –º–æ–¥–µ–ª—å):**
- **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å**: RuBERT –∏–ª–∏ DistilBERT
- **–î–∞—Ç–∞—Å–µ—Ç**: 5K —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
- **–†–∞–∑–º–µ—Ç–∫–∞**: 0 (–Ω–µ–≤–∞–∂–Ω–æ) - 1 (–∫—Ä–∏—Ç–∏—á–Ω–æ)
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 4-6 —á–∞—Å–æ–≤ –Ω–∞ V100
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: ~$100
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 85-90% accuracy

#### **2.2 –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π (Named Entity Recognition)**

**Self-hosted –≤–∞—Ä–∏–∞–Ω—Ç:**
```python
import spacy
from transformers import AutoTokenizer, AutoModelForTokenClassification

class EntityExtractor:
    def __init__(self):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        self.nlp = spacy.load("ru_core_news_sm")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.custom_model = AutoModelForTokenClassification.from_pretrained(
            "DeepPavlov/rubert-base-cased-conversational"
        )
        
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        # –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        doc = self.nlp(text)
        entities = {
            'persons': [],
            'organizations': [],
            'locations': [],
            'dates': [],
            'projects': [],
            'technologies': []
        }
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        for ent in doc.ents:
            if ent.label_ == "PER":
                entities['persons'].append(ent.text)
            elif ent.label_ == "ORG":
                entities['organizations'].append(ent.text)
            elif ent.label_ == "LOC":
                entities['locations'].append(ent.text)
            elif ent.label_ == "DATE":
                entities['dates'].append(ent.text)
        
        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–ø—Ä–æ–µ–∫—Ç—ã, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏)
        custom_entities = self.extract_custom_entities(text)
        entities.update(custom_entities)
        
        return entities
```

**API –≤–∞—Ä–∏–∞–Ω—Ç:**
```python
class APIEntityExtractor:
    def __init__(self):
        self.openai_client = OpenAI()
        
    async def extract_entities(self, text: str) -> Dict[str, List[str]]:
        prompt = f"""
        –ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–ª–µ–¥—É—é—â–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏:
        - –õ—é–¥–∏ (–∏–º–µ–Ω–∞, —É–ø–æ–º–∏–Ω–∞–Ω–∏—è)
        - –ü—Ä–æ–µ–∫—Ç—ã 
        - –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        - –î–∞—Ç—ã –∏ –¥–µ–¥–ª–∞–π–Ω—ã
        - –†–µ—à–µ–Ω–∏—è
        
        –¢–µ–∫—Å—Ç: {text}
        
        –í–µ—Ä–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
        """
        
        response = await self.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        
        return json.loads(response.choices[0].message.content)
```

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤:**
| –ö—Ä–∏—Ç–µ—Ä–∏–π | Self-hosted | API |
|----------|-------------|-----|
| –ö–∞—á–µ—Å—Ç–≤–æ | 80-85% | 90-95% |
| –°—Ç–æ–∏–º–æ—Å—Ç—å | $150/–º–µ—Å—è—Ü | $400/–º–µ—Å—è—Ü |
| –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å | 50-100ms | 200-500ms |
| –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è | –í—ã—Å–æ–∫–∞—è | –ù–∏–∑–∫–∞—è |

#### **2.3 –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class MessageTypeClassifier:
    def __init__(self):
        self.classes = [
            'decision',    # –†–µ—à–µ–Ω–∏–µ
            'question',    # –í–æ–ø—Ä–æ—Å
            'information', # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            'opinion',     # –ú–Ω–µ–Ω–∏–µ
            'task',        # –ó–∞–¥–∞—á–∞
            'casual'       # –û–±—ã—á–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ
        ]
        
    def classify_type(self, message: str) -> str:
        # –ë–µ–∑ –æ–±—É—á–µ–Ω–∏—è - –ø—Ä–∞–≤–∏–ª–∞
        if self.is_decision(message):
            return 'decision'
        elif self.is_question(message):
            return 'question'
        elif self.is_task(message):
            return 'task'
        # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        
    def is_decision(self, message: str) -> bool:
        decision_patterns = [
            r'—Ä–µ—à–∏–ª–∏ —á—Ç–æ',
            r'–ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ',
            r'—É—Ç–≤–µ—Ä–∂–¥–∞–µ–º',
            r'–¥–µ–ª–∞–µ–º —Ç–∞–∫',
            r'–æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ'
        ]
        
        return any(re.search(pattern, message.lower()) for pattern in decision_patterns)
```

**–° –æ–±—É—á–µ–Ω–∏–µ–º:**
- **–ú–æ–¥–µ–ª—å**: DistilBERT –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- **–î–∞—Ç–∞—Å–µ—Ç**: 3K —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
- **–ö–∞—Ç–µ–≥–æ—Ä–∏–∏**: 6 –∫–ª–∞—Å—Å–æ–≤
- **–í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è**: 2 —á–∞—Å–∞ –Ω–∞ V100
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 90-95% accuracy

#### **2.4 –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è**

**Self-hosted —Ä–µ—à–µ–Ω–∏–µ:**
```python
class CustomSummarizer:
    def __init__(self):
        self.model = T5ForConditionalGeneration.from_pretrained(
            "cointegrated/rut5-base-absum"
        )
        self.tokenizer = T5Tokenizer.from_pretrained(
            "cointegrated/rut5-base-absum"
        )
        
    def summarize(self, messages: List[str], max_length: int = 200) -> str:
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        combined_text = " ".join(messages)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        inputs = self.tokenizer(
            combined_text,
            max_length=1024,
            truncation=True,
            return_tensors="pt"
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∑—é–º–µ
        summary_ids = self.model.generate(
            inputs["input_ids"],
            max_length=max_length,
            num_beams=4,
            early_stopping=True
        )
        
        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary
```

**API —Ä–µ—à–µ–Ω–∏–µ:**
```python
class APISummarizer:
    def __init__(self):
        self.openai_client = OpenAI()
        
    async def summarize(self, messages: List[str], max_length: int = 200) -> str:
        combined_text = "\n".join(messages)
        
        prompt = f"""
        –°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —á–∞—Ç–∞:
        
        {combined_text}
        
        –†–µ–∑—é–º–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ –±–æ–ª–µ–µ {max_length} —Å–ª–æ–≤ –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
        - –û—Å–Ω–æ–≤–Ω—ã–µ –æ–±—Å—É–∂–¥–∞–µ–º—ã–µ —Ç–µ–º—ã
        - –ü—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è
        - –í–∞–∂–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        """
        
        response = await self.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=300,
            temperature=0.3
        )
        
        return response.choices[0].message.content
```

---

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 3: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã

### üéØ **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**
–°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø–æ–∏—Å–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

### üîç **–¢–∏–ø—ã –∏–Ω–¥–µ–∫—Å–æ–≤**

#### **3.1 –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å (Temporal Index)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class TemporalIndex:
    def __init__(self):
        self.time_buckets = {
            'hour': timedelta(hours=1),
            'day': timedelta(days=1),
            'week': timedelta(weeks=1),
            'month': timedelta(days=30)
        }
        
    def create_temporal_structure(self, messages: List[Dict]) -> Dict:
        temporal_data = {}
        
        for message in messages:
            timestamp = datetime.fromisoformat(message['timestamp'])
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
            hour_key = timestamp.replace(minute=0, second=0, microsecond=0)
            day_key = timestamp.replace(hour=0, minute=0, second=0, microsecond=0)
            week_key = self.get_week_start(timestamp)
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º
            if hour_key not in temporal_data:
                temporal_data[hour_key] = []
            temporal_data[hour_key].append(message)
            
        return temporal_data
    
    async def search_by_time(self, query: str, time_range: Tuple[datetime, datetime]) -> List[Dict]:
        start_time, end_time = time_range
        
        # –ò—â–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –±–ª–æ–∫–∞—Ö
        relevant_messages = []
        current_time = start_time
        
        while current_time <= end_time:
            hour_messages = await self.get_hour_messages(current_time)
            if hour_messages:
                filtered = self.filter_by_query(hour_messages, query)
                relevant_messages.extend(filtered)
            current_time += timedelta(hours=1)
            
        return relevant_messages
```

**Elasticsearch —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# Mapping –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
temporal_mapping = {
    "mappings": {
        "properties": {
            "timestamp": {
                "type": "date",
                "format": "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
            },
            "content": {
                "type": "text",
                "analyzer": "russian"
            },
            "hour_bucket": {
                "type": "date",
                "format": "yyyy-MM-dd'T'HH:00:00.000'Z'"
            },
            "day_bucket": {
                "type": "date",
                "format": "yyyy-MM-dd'T'00:00:00.000'Z'"
            },
            "team_id": {
                "type": "keyword"
            }
        }
    }
}

class ElasticsearchTemporalIndex:
    def __init__(self):
        self.es_client = Elasticsearch([{'host': 'localhost', 'port': 9200}])
        self.index_name = 'messages_temporal'
        
    async def search_by_time_range(self, query: str, team_id: str, 
                                   start_time: datetime, end_time: datetime) -> List[Dict]:
        
        search_body = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"content": query}},
                        {"term": {"team_id": team_id}},
                        {"range": {
                            "timestamp": {
                                "gte": start_time.isoformat(),
                                "lte": end_time.isoformat()
                            }
                        }}
                    ]
                }
            },
            "sort": [
                {"timestamp": {"order": "desc"}}
            ]
        }
        
        response = await self.es_client.search(
            index=self.index_name,
            body=search_body
        )
        
        return [hit['_source'] for hit in response['hits']['hits']]
```

#### **3.2 –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å (Topical Index)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class TopicalIndex:
    def __init__(self):
        self.topic_hierarchy = {
            'development': ['backend', 'frontend', 'mobile', 'devops'],
            'product': ['features', 'design', 'testing', 'release'],
            'business': ['strategy', 'marketing', 'sales', 'finance'],
            'operations': ['meetings', 'planning', 'reviews', 'admin']
        }
        
    def build_topic_index(self, messages: List[Dict]) -> Dict[str, List[Dict]]:
        topic_index = {}
        
        for message in messages:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—ã —Å–æ–æ–±—â–µ–Ω–∏—è
            topics = self.detect_message_topics(message['content'])
            
            for topic in topics:
                if topic not in topic_index:
                    topic_index[topic] = []
                topic_index[topic].append(message)
                
        return topic_index
    
    def detect_message_topics(self, content: str) -> List[str]:
        topics = []
        content_lower = content.lower()
        
        # –ò—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for main_topic, subtopics in self.topic_hierarchy.items():
            for subtopic in subtopics:
                if subtopic in content_lower:
                    topics.append(f"{main_topic}.{subtopic}")
                    
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–µ–º, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â—É—é
        if not topics:
            topics = self.classify_general_topic(content)
            
        return topics
```

**–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π:**
```python
class VectorTopicalIndex:
    def __init__(self):
        self.pinecone_client = pinecone.Index(host=settings.pinecone_host)
        self.embedding_model = OpenAIEmbeddings()
        
    async def search_by_topic(self, query: str, team_id: str, 
                             topics: List[str] = None) -> List[Dict]:
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = await self.embedding_model.embed_query(query)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ —Ç–µ–º–∞–º
        metadata_filter = {
            "team_id": team_id
        }
        
        if topics:
            metadata_filter["topic"] = {"$in": topics}
        
        # –ü–æ–∏—Å–∫ –≤ Pinecone
        results = self.pinecone_client.query(
            namespace=f"topical-{team_id}",
            vector=query_embedding,
            filter=metadata_filter,
            top_k=10,
            include_metadata=True
        )
        
        return [
            {
                "content": match.metadata["text"],
                "score": match.score,
                "topic": match.metadata["topic"],
                "timestamp": match.metadata["timestamp"]
            }
            for match in results.matches
        ]
```

#### **3.3 –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å (Personal Index)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class PersonalIndex:
    def __init__(self):
        self.user_profiles = {}
        
    def build_personal_index(self, messages: List[Dict]) -> Dict[str, Dict]:
        personal_index = {}
        
        for message in messages:
            author = message['author']
            
            if author not in personal_index:
                personal_index[author] = {
                    'messages': [],
                    'topics': set(),
                    'sentiment': [],
                    'activity_patterns': {}
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            personal_index[author]['messages'].append(message)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            topics = self.extract_user_topics(message['content'])
            personal_index[author]['topics'].update(topics)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ
            sentiment = self.analyze_sentiment(message['content'])
            personal_index[author]['sentiment'].append(sentiment)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            self.update_activity_patterns(personal_index[author], message)
            
        return personal_index
    
    def search_by_person(self, query: str, person: str, team_id: str) -> List[Dict]:
        # –ü–æ–∏—Å–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞
        user_messages = self.get_user_messages(person, team_id)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∑–∞–ø—Ä–æ—Å—É
        relevant_messages = []
        for message in user_messages:
            if self.is_relevant(message['content'], query):
                relevant_messages.append(message)
                
        return relevant_messages
```

#### **3.4 –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω–¥–µ–∫—Å (Factual Index)**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class FactualIndex:
    def __init__(self):
        self.fact_extractors = {
            'decisions': DecisionExtractor(),
            'dates': DateExtractor(),
            'numbers': NumberExtractor(),
            'tasks': TaskExtractor(),
            'links': LinkExtractor()
        }
        
    def extract_facts(self, message: Dict) -> List[Dict]:
        facts = []
        content = message['content']
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Ñ–∞–∫—Ç–æ–≤
        for fact_type, extractor in self.fact_extractors.items():
            extracted = extractor.extract(content)
            
            for fact in extracted:
                facts.append({
                    'type': fact_type,
                    'value': fact['value'],
                    'context': fact['context'],
                    'message_id': message['id'],
                    'timestamp': message['timestamp'],
                    'author': message['author']
                })
                
        return facts
    
    def search_facts(self, query: str, fact_types: List[str] = None) -> List[Dict]:
        # –ü–æ–∏—Å–∫ —Ç–æ–ª—å–∫–æ –ø–æ —Ñ–∞–∫—Ç–∞–º
        search_results = []
        
        for fact in self.fact_database:
            if fact_types and fact['type'] not in fact_types:
                continue
                
            if self.matches_query(fact, query):
                search_results.append(fact)
                
        return search_results
```

**–≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä—ã —Ñ–∞–∫—Ç–æ–≤:**
```python
class DecisionExtractor:
    def __init__(self):
        self.decision_patterns = [
            r'—Ä–µ—à–∏–ª–∏ (.+)',
            r'–ø—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ (.+)',
            r'—É—Ç–≤–µ—Ä–∂–¥–∞–µ–º (.+)',
            r'–¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å (.+)'
        ]
        
    def extract(self, text: str) -> List[Dict]:
        decisions = []
        
        for pattern in self.decision_patterns:
            matches = re.finditer(pattern, text.lower())
            
            for match in matches:
                decisions.append({
                    'value': match.group(1),
                    'context': match.group(0),
                    'confidence': 0.9
                })
                
        return decisions

class DateExtractor:
    def __init__(self):
        self.date_patterns = [
            r'(\d{1,2}\.\d{1,2}\.\d{4})',  # 15.01.2024
            r'(\d{1,2} \w+ \d{4})',        # 15 —è–Ω–≤–∞—Ä—è 2024
            r'(–∑–∞–≤—Ç—Ä–∞|–ø–æ—Å–ª–µ–∑–∞–≤—Ç—Ä–∞|—Å–µ–≥–æ–¥–Ω—è)',
            r'—á–µ—Ä–µ–∑ (\d+) (–¥–Ω|–Ω–µ–¥–µ–ª|–º–µ—Å—è—Ü)'
        ]
        
    def extract(self, text: str) -> List[Dict]:
        dates = []
        
        for pattern in self.date_patterns:
            matches = re.finditer(pattern, text.lower())
            
            for match in matches:
                parsed_date = self.parse_date(match.group(1))
                if parsed_date:
                    dates.append({
                        'value': parsed_date,
                        'context': match.group(0),
                        'confidence': 0.85
                    })
                    
        return dates
```

---

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç 4: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

### üéØ **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ**
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–¥ –∫–∞–∂–¥—ã–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤.

### üß† **–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class QuestionAnalyzer:
    def __init__(self):
        self.question_types = {
            'factual': ['–∫—Ç–æ', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '—Å–∫–æ–ª—å–∫–æ'],
            'procedural': ['–∫–∞–∫', '–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '—Å–ø–æ—Å–æ–±'],
            'causal': ['–ø–æ—á–µ–º—É', '–∏–∑-–∑–∞ —á–µ–≥–æ', '–ø—Ä–∏—á–∏–Ω–∞'],
            'temporal': ['–∫–æ–≥–¥–∞', '–≤–æ —Å–∫–æ–ª—å–∫–æ', '–¥–æ –∫–æ–≥–¥–∞'],
            'comparative': ['–ª—É—á—à–µ', '—Ö—É–∂–µ', '—Å—Ä–∞–≤–Ω–∏'],
            'summary': ['—Ä–µ–∑—é–º–µ', '–∏—Ç–æ–≥–∏', '–æ–±–∑–æ—Ä', 'summary']
        }
        
    def analyze_question(self, question: str) -> Dict:
        analysis = {
            'type': self.detect_question_type(question),
            'entities': self.extract_entities(question),
            'time_scope': self.detect_time_scope(question),
            'context_requirements': self.assess_context_needs(question),
            'priority_sources': self.determine_priority_sources(question)
        }
        
        return analysis
    
    def detect_question_type(self, question: str) -> str:
        question_lower = question.lower()
        
        for q_type, keywords in self.question_types.items():
            if any(keyword in question_lower for keyword in keywords):
                return q_type
                
        return 'general'
    
    def detect_time_scope(self, question: str) -> str:
        time_indicators = {
            'recent': ['—Å–µ–π—á–∞—Å', '—Å–µ–≥–æ–¥–Ω—è', '–Ω–µ–¥–∞–≤–Ω–æ', '–ø–æ—Å–ª–µ–¥–Ω–∏–µ'],
            'specific': ['–≤—á–µ—Ä–∞', '–Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ', '–≤ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫'],
            'period': ['–∑–∞ –º–µ—Å—è—Ü', '–∑–∞ –Ω–µ–¥–µ–ª—é', '–∑–∞ –≥–æ–¥'],
            'all_time': ['–≤—Å–µ–≥–¥–∞', '–∑–∞ –≤—Å–µ –≤—Ä–µ–º—è', '–∏—Å—Ç–æ—Ä–∏—è']
        }
        
        question_lower = question.lower()
        
        for scope, indicators in time_indicators.items():
            if any(indicator in question_lower for indicator in indicators):
                return scope
                
        return 'general'
```

### üìä **–°–±–æ—Ä—â–∏–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class ContextBuilder:
    def __init__(self):
        self.max_context_tokens = 3000
        self.context_sources = {
            'live': LiveContextSource(),
            'daily': DailySummarySource(),
            'topical': TopicalIndexSource(),
            'personal': PersonalIndexSource(),
            'factual': FactualIndexSource()
        }
        
    async def build_context(self, question: str, team_id: str, 
                           question_analysis: Dict) -> Dict:
        
        context_parts = []
        remaining_tokens = self.max_context_tokens
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —Å–±–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        strategy = self.determine_context_strategy(question_analysis)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        for source_name, priority in strategy.items():
            if remaining_tokens <= 0:
                break
                
            source = self.context_sources[source_name]
            source_context = await source.get_context(
                question, team_id, question_analysis, remaining_tokens
            )
            
            if source_context:
                context_parts.append({
                    'source': source_name,
                    'priority': priority,
                    'content': source_context['content'],
                    'tokens_used': source_context['tokens']
                })
                
                remaining_tokens -= source_context['tokens']
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        optimized_context = self.optimize_context(context_parts)
        
        return optimized_context
    
    def determine_context_strategy(self, analysis: Dict) -> Dict[str, int]:
        question_type = analysis['type']
        time_scope = analysis['time_scope']
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
        strategy = {
            'live': 1,
            'daily': 2,
            'topical': 3,
            'personal': 4,
            'factual': 5
        }
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–æ–¥ —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞
        if question_type == 'factual':
            strategy['factual'] = 1
            strategy['live'] = 2
            
        elif question_type == 'temporal':
            if time_scope == 'recent':
                strategy['live'] = 1
            elif time_scope == 'specific':
                strategy['daily'] = 1
                
        elif question_type == 'summary':
            strategy['daily'] = 1
            strategy['topical'] = 2
            
        return strategy
```

### üîß **–ö–æ–º–ø—Ä–µ—Å—Å–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class ContextCompressor:
    def __init__(self):
        self.compression_strategies = {
            'summarization': SummarizationCompressor(),
            'extraction': ExtractionCompressor(),
            'filtering': FilteringCompressor(),
            'clustering': ClusteringCompressor()
        }
        
    def compress_context(self, context_parts: List[Dict], 
                        target_tokens: int) -> Dict:
        
        total_tokens = sum(part['tokens_used'] for part in context_parts)
        
        if total_tokens <= target_tokens:
            return self.merge_context_parts(context_parts)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏
        compression_ratio = target_tokens / total_tokens
        
        if compression_ratio > 0.7:
            # –õ–µ–≥–∫–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            compressed = self.compression_strategies['filtering'].compress(
                context_parts, target_tokens
            )
        elif compression_ratio > 0.4:
            # –°—Ä–µ–¥–Ω—è—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —á–∞—Å—Ç–µ–π
            compressed = self.compression_strategies['extraction'].compress(
                context_parts, target_tokens
            )
        else:
            # –°–∏–ª—å–Ω–∞—è –∫–æ–º–ø—Ä–µ—Å—Å–∏—è - —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
            compressed = self.compression_strategies['summarization'].compress(
                context_parts, target_tokens
            )
            
        return compressed

class SummarizationCompressor:
    def __init__(self):
        self.summarizer = T5ForConditionalGeneration.from_pretrained(
            "cointegrated/rut5-base-absum"
        )
        
    def compress(self, context_parts: List[Dict], target_tokens: int) -> Dict:
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ —á–∞—Å—Ç–∏
        grouped_parts = self.group_similar_parts(context_parts)
        
        compressed_parts = []
        tokens_used = 0
        
        for group in grouped_parts:
            if tokens_used >= target_tokens:
                break
                
            # –°—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—É
            group_text = " ".join([part['content'] for part in group])
            summary = self.summarize_text(group_text, max_length=200)
            
            summary_tokens = len(summary.split())
            if tokens_used + summary_tokens <= target_tokens:
                compressed_parts.append({
                    'content': summary,
                    'tokens': summary_tokens,
                    'compression_ratio': len(group_text) / len(summary)
                })
                tokens_used += summary_tokens
                
        return {
            'content': compressed_parts,
            'total_tokens': tokens_used,
            'compression_method': 'summarization'
        }
```

### ‚úÖ **–í–∞–ª–∏–¥–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class AnswerValidator:
    def __init__(self):
        self.validation_criteria = {
            'relevance': RelevanceValidator(),
            'completeness': CompletenessValidator(),
            'accuracy': AccuracyValidator(),
            'coherence': CoherenceValidator()
        }
        
    async def validate_answer(self, question: str, answer: str, 
                            context: Dict) -> Dict:
        
        validation_results = {}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –≤—Å–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        for criterion, validator in self.validation_criteria.items():
            score = await validator.validate(question, answer, context)
            validation_results[criterion] = score
            
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        overall_score = sum(validation_results.values()) / len(validation_results)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —É–ª—É—á—à–∞—Ç—å –æ—Ç–≤–µ—Ç
        needs_improvement = overall_score < 0.7
        
        return {
            'scores': validation_results,
            'overall_score': overall_score,
            'needs_improvement': needs_improvement,
            'suggestions': self.generate_improvement_suggestions(validation_results)
        }

class RelevanceValidator:
    def __init__(self):
        self.similarity_model = SentenceTransformer('all-MiniLM-L6-v2')
        
    async def validate(self, question: str, answer: str, context: Dict) -> float:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å
        question_embedding = self.similarity_model.encode(question)
        answer_embedding = self.similarity_model.encode(answer)
        
        similarity = cosine_similarity([question_embedding], [answer_embedding])[0][0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Ç–≤–µ—á–∞–µ—Ç –ª–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        contains_answer = self.contains_direct_answer(question, answer)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        relevance_score = (similarity * 0.6) + (contains_answer * 0.4)
        
        return relevance_score
```

---

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è

### üé≠ **–û—Å–Ω–æ–≤–Ω–æ–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä**

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
class RAGOrchestrator:
    def __init__(self):
        self.hierarchical_context = HierarchicalContextManager()
        self.data_processor = DataPreprocessor()
        self.multi_index = MultiIndexManager()
        self.dynamic_context = DynamicContextBuilder()
        
    async def process_question(self, question: str, team_id: str, 
                              user_id: str) -> Dict:
        
        # –≠—Ç–∞–ø 1: –ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞
        question_analysis = await self.analyze_question(question)
        
        # –≠—Ç–∞–ø 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞
        search_strategy = self.determine_search_strategy(question_analysis)
        
        # –≠—Ç–∞–ø 3: –°–±–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = await self.build_optimized_context(
            question, team_id, question_analysis, search_strategy
        )
        
        # –≠—Ç–∞–ø 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        answer = await self.generate_answer(question, context)
        
        # –≠—Ç–∞–ø 5: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ
        validation = await self.validate_answer(question, answer, context)
        
        if validation['needs_improvement']:
            # –£–ª—É—á—à–∞–µ–º –æ—Ç–≤–µ—Ç
            improved_answer = await self.improve_answer(
                question, answer, context, validation['suggestions']
            )
            answer = improved_answer
            
        # –≠—Ç–∞–ø 6: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ
        await self.log_interaction(question, answer, context, validation)
        
        return {
            'answer': answer,
            'confidence': validation['overall_score'],
            'context_used': context['summary'],
            'processing_time': context['processing_time']
        }
```

---

## –°—Ç—Ä–∞—Ç–µ–≥–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ fine-tuning

### üéì **–ß—Ç–æ –Ω—É–∂–Ω–æ –æ–±—É—á–∞—Ç—å**

#### **1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏–π**
- **–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å**: –í—ã—Å–æ–∫–∞—è
- **–î–∞–Ω–Ω—ã–µ**: 5K —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
- **–í—Ä–µ–º—è**: 4-6 —á–∞—Å–æ–≤
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: $100-150
- **–£–ª—É—á—à–µ–Ω–∏–µ**: +15% —Ç–æ—á–Ω–æ—Å—Ç–∏

#### **2. –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π**
- **–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å**: –°—Ä–µ–¥–Ω—è—è
- **–î–∞–Ω–Ω—ã–µ**: 3K —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
- **–í—Ä–µ–º—è**: 6-8 —á–∞—Å–æ–≤
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: $200-300
- **–£–ª—É—á—à–µ–Ω–∏–µ**: +20% —Ç–æ—á–Ω–æ—Å—Ç–∏

#### **3. –ú–æ–¥–µ–ª—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏**
- **–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å**: –í—ã—Å–æ–∫–∞—è
- **–î–∞–Ω–Ω—ã–µ**: 2K –ø–∞—Ä (—Å–æ–æ–±—â–µ–Ω–∏—è -> —Ä–µ–∑—é–º–µ)
- **–í—Ä–µ–º—è**: 8-12 —á–∞—Å–æ–≤
- **–°—Ç–æ–∏–º–æ—Å—Ç—å**: $300-500
- **–£–ª—É—á—à–µ–Ω–∏–µ**: +25% –∫–∞—á–µ—Å—Ç–≤–∞

### üöÄ **–ß—Ç–æ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è**

#### **1. –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫**
- **–ì–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è**: Elasticsearch, Pinecone
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 80-85%
- **–í—Ä–µ–º—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è**: 1-2 –Ω–µ–¥–µ–ª–∏

#### **2. –ë–∞–∑–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π**
- **–ü–æ–¥—Ö–æ–¥**: –ü—Ä–∞–≤–∏–ª–∞ + –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 70-75%
- **–í—Ä–µ–º—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è**: 3-5 –¥–Ω–µ–π

#### **3. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫**
- **–ü–æ–¥—Ö–æ–¥**: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∞–≤—Ç–æ—Ä—É + —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 75-80%
- **–í—Ä–µ–º—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è**: 1 –Ω–µ–¥–µ–ª—è

### üìà **–ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ**

#### **–≠—Ç–∞–ø 1: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π MVP (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è)**
- –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
- –ì–æ—Ç–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- **–í—Ä–µ–º—è**: 4-6 –Ω–µ–¥–µ–ª—å
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 70-75%

#### **–≠—Ç–∞–ø 2: –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (—á–∞—Å—Ç–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)**
- Fine-tuned –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–∞–∂–Ω–æ—Å—Ç–∏
- –ö–∞—Å—Ç–æ–º–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π
- **–í—Ä–µ–º—è**: +2-3 –Ω–µ–¥–µ–ª–∏
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 80-85%

#### **–≠—Ç–∞–ø 3: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–µ—Ä—Å–∏—è (–ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ)**
- –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω—ã –Ω–∞ –¥–æ–º–µ–Ω–µ
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
- **–í—Ä–µ–º—è**: +3-4 –Ω–µ–¥–µ–ª–∏
- **–ö–∞—á–µ—Å—Ç–≤–æ**: 85-90%

### üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–Ω–µ–¥—Ä–µ–Ω–∏—é**

1. **–ù–∞—á–Ω–∏—Ç–µ —Å MVP**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≥–æ—Ç–æ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
2. **–°–æ–±–∏—Ä–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ**: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
3. **–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ**: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –∑–∞–º–µ–Ω—è–π—Ç–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ –æ–±—É—á–µ–Ω–Ω—ã–µ
4. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞**: –ü–æ—Å—Ç–æ—è–Ω–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏
5. **–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ñ–∏–¥–±–µ–∫–∏**: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ—Ç–∑—ã–≤—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è

**–ò—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: –ù–∞—á–Ω–∏—Ç–µ —Å –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –¥–∞—Å—Ç 70-75% –∫–∞—á–µ—Å—Ç–≤–∞, –∞ –∑–∞—Ç–µ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–ª—É—á—à–∞–π—Ç–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ fine-tuning –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è 85-90% –∫–∞—á–µ—Å—Ç–≤–∞. 