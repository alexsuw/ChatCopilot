# Настройка vLLM для ChatCopilot

## Что изменилось

Проект переведен с **Google Gemini** на **vLLM** с вашей собственной моделью.

### Удаленные компоненты:
- `google-generativeai` зависимость
- `GOOGLE_API_KEY` переменная окружения
- Все настройки Gemini safety_settings

### Добавленные компоненты:
- HTTP клиент для работы с vLLM API
- Переменные окружения для vLLM
- Функции проверки здоровья vLLM сервера
- Улучшенная обработка ошибок

## Настройка переменных окружения

Обновите ваш `.env` файл:

```bash
# vLLM Configuration
VLLM_URL=http://localhost:8000
VLLM_MODEL_NAME=Qwen/Qwen3-0.6B
VLLM_MAX_TOKENS=2048
VLLM_TEMPERATURE=0.7
VLLM_TIMEOUT=30
```

### Описание переменных:

- `VLLM_URL` - URL вашего vLLM сервера (по умолчанию: localhost:8000)
- `VLLM_MODEL_NAME` - название модели в vLLM (например: Qwen/Qwen3-0.6B)
- `VLLM_MAX_TOKENS` - максимальное количество токенов в ответе
- `VLLM_TEMPERATURE` - температура для генерации (0.0 - 1.0)
- `VLLM_TIMEOUT` - таймаут для HTTP запросов в секундах

## Запуск vLLM сервера

Убедитесь, что ваш vLLM сервер запущен:

```bash
# Пример запуска vLLM сервера
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-0.6B \
    --port 8000 \
    --host 0.0.0.0
```

## Тестирование

### 1. Тест подключения к vLLM:
```bash
python test_vllm.py
```

### 2. Полная диагностика системы:
```bash
python diagnostic.py
```

### 3. Проверка через curl:
```bash
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen3-0.6B",
        "prompt": "Привет! Как дела?",
        "max_tokens": 100,
        "temperature": 0.7
    }'
```

## Изменения в коде

### Основные файлы:
- `src/services/llm.py` - полностью переписан для vLLM
- `src/settings.py` - добавлены настройки vLLM
- `requirements.txt` - убран google-generativeai
- `diagnostic.py` - обновлены проверки
- `env.example` - новые переменные окружения

### Новые функции:
- `get_answer()` - работает с vLLM API
- `check_vllm_health()` - проверка состояния сервера
- `test_vllm_simple()` - простой тест генерации

## Возможные проблемы

### 1. Сервер vLLM недоступен
```
❌ Не удалось подключиться к серверу ИИ. Проверьте, что vLLM запущен.
```
**Решение:** Убедитесь, что vLLM сервер запущен на указанном URL.

### 2. Неверная модель
```
❌ Неверные параметры запроса. Обратитесь к администратору.
```
**Решение:** Проверьте, что `VLLM_MODEL_NAME` соответствует загруженной модели.

### 3. Превышение времени ожидания
```
❌ Превышено время ожидания. Попробуйте сократить вопрос.
```
**Решение:** Увеличьте `VLLM_TIMEOUT` или оптимизируйте запросы.

## Производительность

- Система поддерживает повторные попытки (3 попытки)
- Экспоненциальный backoff для повторных запросов
- Подробное логирование для отладки
- Проверка здоровья сервера перед запросами 